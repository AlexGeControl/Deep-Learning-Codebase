{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tensorflow Utilities Tutorial\n",
    "\n",
    "***\n",
    "\n",
    "In this tutorial, the programming exercises from Chapter 9. Up and Running with Tensorflow of [Hands-On Machine Learning with Scikit-Learn and Tensorflow](https://www.safaribooksonline.com/library/view/hands-on-machine-learning/9781491962282/) will be refactored and reproduced.\n",
    "\n",
    "By finishing this tutorial, you'll know the basics and paradigms of programming in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Up Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basics of Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Every node is associated with a computational graph:\n",
    "with tf.Graph().as_default():\n",
    "    x = tf.Variable(42, name='x')\n",
    "\n",
    "x.graph is tf.get_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Result]: 77\n"
     ]
    }
   ],
   "source": [
    "# Initialize graph:\n",
    "with tf.Graph().as_default():\n",
    "    # Construction phase:\n",
    "    x = tf.Variable(3, name='x')\n",
    "    y = tf.Variable(4, name='y')\n",
    "\n",
    "    f = x**3 + x**2*y + x*y + 2\n",
    "\n",
    "    init = tf.global_variables_initializer()\n",
    "\n",
    "    # Execution phase:\n",
    "    with tf.Session() as sess:\n",
    "        # Inside the with block, the newly created sess is used as the default session:\n",
    "        init.run()        #tf.get_default_session.run(init)\n",
    "        result = f.eval() #tf.get_default_session.run(f)\n",
    "\n",
    "    print(\"[Result]: {}\".format(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Y]: 16, [Z]: 64\n"
     ]
    }
   ],
   "source": [
    "# Evaluate as much as possible in one graph run:\n",
    "with tf.Graph().as_default():\n",
    "    # Construction phase:\n",
    "    x = tf.Variable(3, name='x')\n",
    "    \n",
    "    y = (x + 1)**2\n",
    "    z = (x + 1)**3\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    # Execution phase:\n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        y_val, z_val = sess.run([y, z])\n",
    "    \n",
    "    print(\"[Y]: {}, [Z]: {}\".format(y_val, z_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression through Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "california_housing = fetch_california_housing()\n",
    "\n",
    "# Features:\n",
    "features = StandardScaler().fit_transform(\n",
    "    california_housing.data\n",
    ")\n",
    "# Targets:\n",
    "targets = california_housing.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9, 20640) (1, 20640)\n"
     ]
    }
   ],
   "source": [
    "# Feature dimensions:\n",
    "N, D = features.shape\n",
    "\n",
    "# Follow Andrew Ng's convensions:\n",
    "features = np.c_[features, np.ones((N, 1))]\n",
    "features = features.T\n",
    "\n",
    "targets = targets.reshape((-1, 1)).T\n",
    "\n",
    "# Finally:\n",
    "print(repr(features.shape), repr(targets.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get Optimal Weights using Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss]: 0.5243210196495056\n",
      "[R-Squared]: 60.62326600675225\n"
     ]
    }
   ],
   "source": [
    "# Use normal equation from multivariate analysis:\n",
    "with tf.Graph().as_default():\n",
    "    X = tf.constant(features, dtype=tf.float32, name='X')\n",
    "    y = tf.constant(targets, dtype=tf.float32, name='y')\n",
    "    \n",
    "    X_transposed = tf.transpose(X)\n",
    "    w = tf.matmul(\n",
    "        y, tf.matmul(\n",
    "            X_transposed, tf.matrix_inverse(\n",
    "                tf.matmul(X, X_transposed)\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        (tf.matmul(w, X) - y)**2\n",
    "    )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        w_val, loss_val = sess.run([w, loss])\n",
    "        \n",
    "    print(\"[Loss]: {}\".format(loss_val))\n",
    "    \n",
    "    total_variance = np.var(targets)\n",
    "    print(\n",
    "        \"[R-Squared]: {}\".format(\n",
    "            100 * (1.0 - loss_val / total_variance)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Optimal Weights using Simple Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Only small learning rate could be used:\n",
    "learning_rate = 0.01\n",
    "# Need more iterations to converge:\n",
    "max_iters = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss @ 250]: 0.5808319058927106\n",
      "[Loss @ 500]: 0.5492083942066661\n",
      "[Loss @ 750]: 0.5355157712908981\n",
      "[Loss @ 1000]: 0.5293930842553165\n",
      "[R-Squared]: 60.243549419289344\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    X = tf.constant(features, dtype=tf.float64, name='X')\n",
    "    y = tf.constant(targets, dtype=tf.float64, name='y')\n",
    "    \n",
    "    w = tf.Variable(\n",
    "        np.sqrt(1/(D+1)) * np.random.randn(1, D+1),\n",
    "        name='w'\n",
    "    )\n",
    "    \n",
    "    y_pred = tf.matmul(w, X, name='prediction')\n",
    "    error = y_pred - y\n",
    "    \n",
    "    loss = tf.reduce_mean(error**2, name='loss')\n",
    "    \n",
    "    dw = 2.0 / N * tf.matmul(\n",
    "        error,\n",
    "        tf.transpose(X)\n",
    "    )\n",
    "    optimization = tf.assign(w, w - learning_rate*dw)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            _, loss_val = sess.run([optimization, loss])\n",
    "            \n",
    "            if (i + 1) % 250 == 0:\n",
    "                print(\n",
    "                    \"[Loss @ {}]: {}\".format(\n",
    "                        i + 1,\n",
    "                        loss_val\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        w_optimal = w.eval()\n",
    "        loss_optimal = loss.eval()\n",
    "    \n",
    "        print(\n",
    "            \"[R-Squared]: {}\".format(\n",
    "                100 * (1.0 - loss_optimal / np.var(targets))\n",
    "            )\n",
    "        )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Optimal Weights using Adam Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Larger learning rates can be used:\n",
    "learning_rate = 0.1\n",
    "# And converge faster:\n",
    "max_iters = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss @ 50]: 0.5300696547593745\n",
      "[Loss @ 100]: 0.5244123209869164\n",
      "[Loss @ 150]: 0.5243210139187467\n",
      "[Loss @ 200]: 0.5243209868658081\n",
      "[R-Squared]: 60.62326848676274\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    X = tf.constant(features, dtype=tf.float64, name='X')\n",
    "    y = tf.constant(targets, dtype=tf.float64, name='y')\n",
    "    \n",
    "    w = tf.Variable(\n",
    "        np.sqrt(1/(D+1)) * np.random.randn(1, D+1),\n",
    "        name='w'\n",
    "    )\n",
    "    y_pred = tf.matmul(w, X, name='prediction')\n",
    "    \n",
    "    loss = tf.reduce_mean((y_pred - y)**2, name='loss')\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        \n",
    "        for i in range(max_iters):\n",
    "            _, loss_val = sess.run([optimizer, loss])\n",
    "            \n",
    "            if (i + 1) % 50 == 0:\n",
    "                print(\n",
    "                    \"[Loss @ {}]: {}\".format(\n",
    "                        i + 1,\n",
    "                        loss_val\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        w_optimal = w.eval()\n",
    "        loss_optimal = loss.eval()\n",
    "    \n",
    "        print(\n",
    "            \"[R-Squared]: {}\".format(\n",
    "                100 * (1.0 - loss_optimal / np.var(targets))\n",
    "            )\n",
    "        )    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Optimal Weights using Mini-Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D, N = features.shape\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Loss @ 10]: 0.3948565401263247\n",
      "[Loss @ 20]: 0.5279167089556231\n",
      "[R-Squared]: 59.462623860017814\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    X = tf.placeholder(shape=(D, None), dtype=tf.float64, name='X')\n",
    "    y = tf.placeholder(shape=(1, None), dtype=tf.float64, name='y')\n",
    "    \n",
    "    w = tf.Variable(\n",
    "        np.sqrt(1 / D) * np.random.randn(1, D),\n",
    "        name = 'w'\n",
    "    )\n",
    "    \n",
    "    y_pred = tf.matmul(w, X, name='prediction')\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "        (y_pred - y)**2,\n",
    "        name = 'loss'\n",
    "    )\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            shuffled_indices = np.random.permutation(N)\n",
    "            \n",
    "            for b in range(N // batch_size + 1):\n",
    "                batch_indices = shuffled_indices[b*batch_size: (b+1)*batch_size]\n",
    "                X_batch = features[:, batch_indices]\n",
    "                y_batch = targets[:, batch_indices]\n",
    "                _, loss_val = sess.run(\n",
    "                    [optimizer, loss],\n",
    "                    feed_dict = {\n",
    "                        X: X_batch,\n",
    "                        y: y_batch\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            if (e + 1) % 10 == 0:\n",
    "                print(\n",
    "                    \"[Loss @ {}]: {}\".format(\n",
    "                        e + 1,\n",
    "                        loss_val\n",
    "                    )\n",
    "                )\n",
    "        \n",
    "        # Finally:\n",
    "        loss_optimal = sess.run(\n",
    "            loss,\n",
    "            feed_dict = {\n",
    "                X: features,\n",
    "                y: targets\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(\n",
    "        \"[R-Squared]: {}\".format(\n",
    "            100 * (1.0 - loss_optimal / np.var(targets))\n",
    "        )\n",
    "    )     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use TensorBoard for Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Configurations:\n",
    "D, N = features.shape\n",
    "\n",
    "learning_rate = 0.1\n",
    "\n",
    "epochs = 20\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Log output:\n",
    "from datetime import datetime\n",
    "\n",
    "timestamp = datetime.utcnow().strftime(\"%Y%m%d%H%M%S\")\n",
    "log_root_dir = \"tf-logs\"\n",
    "log_dir = \"{}/run-{}/\".format(log_root_dir, timestamp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[R-Squared]: 53.3472802987882\n"
     ]
    }
   ],
   "source": [
    "with tf.Graph().as_default():\n",
    "    with tf.name_scope('input'):\n",
    "        X = tf.placeholder(shape=(D, None), dtype=tf.float64, name='X')\n",
    "        y = tf.placeholder(shape=(1, None), dtype=tf.float64, name='y')\n",
    "    \n",
    "    with tf.name_scope('params'):\n",
    "        w = tf.Variable(\n",
    "            np.sqrt(1 / D) * np.random.randn(1, D),\n",
    "            name = 'w'\n",
    "        )\n",
    "\n",
    "    with tf.name_scope('output'):\n",
    "        y_pred = tf.matmul(w, X, name='prediction')\n",
    "    \n",
    "    with tf.name_scope('metrics'):\n",
    "        loss = tf.reduce_mean(\n",
    "            (y_pred - y)**2,\n",
    "            name = 'loss'\n",
    "        )\n",
    "    \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "    \n",
    "    with tf.name_scope('logging'):\n",
    "        loss_summary = tf.summary.scalar('Loss', loss)\n",
    "        log_writer = tf.summary.FileWriter(\n",
    "            log_dir, \n",
    "            tf.get_default_graph()\n",
    "        )\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        init.run()\n",
    "        \n",
    "        for e in range(epochs):\n",
    "            shuffled_indices = np.random.permutation(N)\n",
    "            \n",
    "            for b in range(N // batch_size + 1):\n",
    "                batch_indices = shuffled_indices[b*batch_size: (b+1)*batch_size]\n",
    "                X_batch = features[:, batch_indices]\n",
    "                y_batch = targets[:, batch_indices]\n",
    "                _ = sess.run(\n",
    "                    optimizer,\n",
    "                    feed_dict = {\n",
    "                        X: X_batch,\n",
    "                        y: y_batch\n",
    "                    }\n",
    "                )\n",
    "            \n",
    "            if (e + 1) % 2 == 0:\n",
    "                loss_val = loss_summary.eval(\n",
    "                    feed_dict = {\n",
    "                        X: features,\n",
    "                        y: targets                        \n",
    "                    }\n",
    "                )\n",
    "                log_writer.add_summary(\n",
    "                    loss_val, e + 1\n",
    "                )\n",
    "        \n",
    "        # Finally:\n",
    "        loss_optimal = sess.run(\n",
    "            loss,\n",
    "            feed_dict = {\n",
    "                X: features,\n",
    "                y: targets\n",
    "            }\n",
    "        )\n",
    "\n",
    "    log_writer.close()\n",
    "    \n",
    "    print(\n",
    "        \"[R-Squared]: {}\".format(\n",
    "            100 * (1.0 - loss_optimal / np.var(targets))\n",
    "        )\n",
    "    )     "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
